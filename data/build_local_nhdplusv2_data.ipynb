{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook imports a table of VAA attributes of all Lower 48 network stream segments in the NHDPlusV2.1.  We subset these data into a set of attributes that we know we need for network operations and pickle these data for fast read access for future steps.  We also pickle all attributes for other efforts where we may need additional attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in source data and build binary format for quick reading of data\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import urllib.request as ur\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flow_table.zip', <http.client.HTTPMessage at 0x170bd362048>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download data from ScienceBase source: https://www.sciencebase.gov/catalog/item/5d126fa3e4b0941bde56ea73\n",
    "download_url = 'https://www.sciencebase.gov/catalog/file/get/5d126fa3e4b0941bde56ea73?f=__disk__c9%2Fad%2F57%2Fc9ad57469af8f37fc1a5577cc75a080cf39ea8b2'\n",
    "\n",
    "\n",
    "download_file = 'flow_table.zip'\n",
    "ur.urlretrieve(download_url, download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data into pandas from zip txt file\n",
    "download_file = 'flow_table.zip'\n",
    "df = pd.read_csv(download_file)\n",
    "\n",
    "#create float version of segment ids (comids as float)\n",
    "df['seg_id'] = df['COMID'].astype(float)\n",
    "df['watershed_id'] =df['VPUID'].astype(str)\n",
    "\n",
    "update_watersheds = {'10U': '10', '10L': '10', '7': '07', '5': '05', '8': '08', '03W': '03', '03N': '03', '2': '02', '4': '04', '1': '01', '6': '06', '03S': '03', '9': '09'}\n",
    "df = df.replace({'watershed_id':update_watersheds})\n",
    "\n",
    "#update nan in rpuid\n",
    "df['proc_unit'] = df['RPUID']\n",
    "df['proc_unit'] = np.where(df.proc_unit.isnull(), (df['watershed_id'].astype(str)+'a'), (df['proc_unit'].astype(str)))\n",
    "\n",
    "#Update watersheds to represent drainages of US \n",
    "#05,06,07,08,10U,10L,11 = MISSISSIPPI\n",
    "#01 = New England\n",
    "#02 = Mid Atlantic\n",
    "#03N, 03S, 03W = South Atlantic Gulf\n",
    "#04 = Great Lakes\n",
    "#09 = Souris Red-Rainy\n",
    "#12 = Texas Gulf\n",
    "#13 = Rio Grande\n",
    "#14, 15 = Colorado\n",
    "#16 = Great Basin\n",
    "#17 = Pacific Northwest\n",
    "#18 = California\n",
    "update_watersheds = {'05':'11','06':'11','07':'11','08':'11','10':'11','14':'15'}\n",
    "df = df.replace({'watershed_id':update_watersheds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'11': 1243141, '03': 330422, '17': 232810, '15': 187008, '18': 142613, '02': 129670, '04': 107692, '16': 96269, '12': 68901, '01': 67906, '13': 55854, '09': 29053})\n"
     ]
    }
   ],
   "source": [
    "#Shows number of records per watershed \n",
    "l = df['watershed_id'].tolist()\n",
    "import collections\n",
    "counter=collections.Counter(l)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create subset of dataset, this subset includes data that are needed for many network operations\n",
    "##################################################################################################\n",
    "#Select needed fields\n",
    "network_df_sub = df[['seg_id','COMID', 'FTYPE' ,'AreaSqKM', 'StreamOrde', 'TotDASqKM', 'REACHCODE', 'LENGTHKM', 'StreamOrde','StartFlag', 'FromMeas', 'ToMeas','FromNode', 'ToNode','watershed_id','proc_unit']]\n",
    "#Remove Coastline flowlines\n",
    "network_df_sub = network_df_sub.loc[network_df_sub['FTYPE']!='Coastline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize subset of data for faster read\n",
    "network_df_sub.to_pickle('flow_table_sub.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize all of the data in case we need additional attributes\n",
    "#df['seg_id'] = df['COMID'].astype(float)\n",
    "df.to_pickle('flow_table_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
