{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example builds hdf files to store network information.  For each stream unit a list of parent segment ids are stored, along with local and network areas.  These variables can then be easily accessed to calculate upstream summaries without rebuilding the stream network for each calculation.  This example works with the NHDPlusV2.1 dataset but has been tested on the NHDPlusHR as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = pd.read_pickle('data/flow_table_sub.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use To From Nodes to build table of all seg_ids and upstream seg ids.  Note many seg_ids will have multiple records, one for each upstream seg_id\n",
    "upid_temp = stream_df[['seg_id','ToNode']]\n",
    "upstream_seg_list = pd.merge(stream_df, upid_temp, left_on='FromNode', right_on='ToNode', how='left')\n",
    "\n",
    "#Rename fields (note if not using NHDPlusV2.1 files make sure you name files accordingly)\n",
    "upstream_seg_list.rename(columns={'seg_id_x':'seg_id','seg_id_y':'upstream_seg_id','LENGTHKM':'len_km','AreaSqKM':'area_sqkm', 'TotDASqKM':'tot_area_sqkm', 'RPUID': 'rpuid', 'VPUID':'vpuid'}, inplace=True)\n",
    "\n",
    "#Drop unneeded fields\n",
    "upstream_seg_list.drop(columns=['FromNode','ToNode_x','ToNode_y'], inplace=True)\n",
    "\n",
    "#Print shape of dataframe to get an idea of if we have the right output or not\n",
    "#upstream_seg_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwieferich\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Fill null upstream segment ids with zeros\n",
    "isnull_df = (upstream_seg_list[upstream_seg_list['upstream_seg_id'].isnull()])\n",
    "isnull_df.fillna(0, inplace=True)\n",
    "\n",
    "isnotnull_df = (upstream_seg_list[upstream_seg_list['upstream_seg_id'].notna()])\n",
    "upstream_seg_list = isnotnull_df.append(isnull_df)\n",
    "\n",
    "#upstream_seg_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete unneeded dataframes and data\n",
    "del isnull_df\n",
    "del isnotnull_df\n",
    "del upid_temp\n",
    "del stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is an issue with a to/from node in NHDPlusV2, this ensures the bad (duplicated) node doesn;t result in false routing\n",
    "upstream_seg_list.shape\n",
    "del_rows = upstream_seg_list[((upstream_seg_list['seg_id']==24561351.0) & (upstream_seg_list['upstream_seg_id']==22227020.0)) | ((upstream_seg_list['seg_id']==24561351.0) & (upstream_seg_list['upstream_seg_id']==23949489.0))].index\n",
    "upstream_seg_list.drop(del_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seg_id', 'COMID', 'FTYPE', 'area_sqkm', 'StreamOrde', 'tot_area_sqkm',\n",
       "       'REACHCODE', 'len_km', 'StreamOrde', 'StartFlag', 'FromMeas', 'ToMeas',\n",
       "       'watershed_id', 'proc_unit', 'upstream_seg_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upstream_seg_list.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region01:289.4623286222535\n",
      "region02:545.8974282580187\n",
      "region03:1453.377068238939\n",
      "region04:429.15438620546774\n",
      "region09:120.49025267414163\n",
      "region11:6974.4573935120425\n",
      "region12:284.37484410259276\n",
      "region13:262.3691435452274\n",
      "region15:881.161270853785\n",
      "region16:401.33782257088205\n",
      "region17:1107.0590225761316\n",
      "region18:600.8853309838705\n",
      "total seconds: 13350.032464222\n"
     ]
    }
   ],
   "source": [
    "#Run Build Network by region, uses floats for ids\n",
    "import time\n",
    "import xstrm.build_network_rpu as build\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "watersheds = ['01','02','03','04','09','11','12','13','15','16','17','18']\n",
    "\n",
    "for watershed in watersheds:\n",
    "    tic = time.clock()\n",
    "    #subset dataframe by drainage\n",
    "    df_infile = upstream_seg_list.loc[upstream_seg_list['watershed_id']==watershed]\n",
    "    #print (df_infile.shape)\n",
    "\n",
    "    traverse_queue = build.upstream_setup(df_infile)\n",
    "    build.upstream_build_network(traverse_queue)\n",
    "    toc = time.clock()\n",
    "    print ('region' +str(watershed) + ':' + str(toc-tic))\n",
    "\n",
    "end_time = time.clock()\n",
    "print ('total seconds: ' + (str(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream_seg_list.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Seconds to process each region when processing entire region.  Although this is faster to build hdf5 files upstream agg calculations are slower due to larger hdf5 file size (entire watershed vs. regional processing unit)\n",
    "\n",
    "region11:3031.6833874993254\n",
    "region14:313.4585673921697\n",
    "region1:70.33057106223168\n",
    "region2:143.75602766841666\n",
    "region3:399.6618969961214\n",
    "region4:106.68949977171542\n",
    "region9:41.31301464378066\n",
    "region12:73.93708053568935\n",
    "region13:81.08523266821976\n",
    "region16:95.8744506211824\n",
    "region17:297.7542863581921\n",
    "region18:148.6522641773645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Shows number of records per watershed \n",
    "l = upstream_seg_list['proc_unit'].tolist()\n",
    "import collections\n",
    "counter=collections.Counter(l)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing code when create hdf5 by regional processing unit (in this test the file opens and closes files every record).  it creates many hdf5 files and takes longer to build, but upstream calc performance is improved greatly\n",
    "\n",
    "region01:289.4623286222535\n",
    "region02:545.8974282580187\n",
    "region03:1453.377068238939\n",
    "region04:429.15438620546774\n",
    "region09:120.49025267414163\n",
    "region11:6974.4573935120425\n",
    "region12:284.37484410259276\n",
    "region13:262.3691435452274\n",
    "region15:881.161270853785\n",
    "region16:401.33782257088205\n",
    "region17:1107.0590225761316\n",
    "region18:600.8853309838705\n",
    "total seconds: 13350.032464222"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
